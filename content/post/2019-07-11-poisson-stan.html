---
title: 泊松回歸模型的貝葉斯Stan實現
author: ''
date: '2019-07-11'
slug: poisson-stan
categories: 
  - Bayesian
  - R techniques
  - statistics
tags: 
  - Bayesian
  - Medical Statistics
header:
  caption: 'Thomas Bayes'
  image: '052816_bayesian-opener_free.jpg'
output:
  blogdown::html_page:
    toc: true
summary: "Rstan 學習筆記 Chapter 5.4"
---


<div id="TOC">
<ul>
<li><a href="#分析目的數據和選擇-poisson-回歸模型的原因">分析目的，數據，和選擇 Poisson 回歸模型的原因</a><ul>
<li><a href="#想象模型機制">想象模型機制</a></li>
<li><a href="#寫下數學模型表達式">寫下數學模型表達式</a></li>
<li><a href="#把數學模型翻譯成-stan-模型代碼">把數學模型翻譯成 Stan 模型代碼</a></li>
<li><a href="#運行結果的解釋">運行結果的解釋</a></li>
</ul></li>
</ul>
</div>

<div id="分析目的數據和選擇-poisson-回歸模型的原因" class="section level1">
<h1>分析目的，數據，和選擇 Poisson 回歸模型的原因</h1>
<p>我們這裏使用<a href="https://wangcc.me/post/logistic-rstan/">之前擬合貝葉斯邏輯回歸模型</a>時使用的相同的數據來展示如何跑貝葉斯泊松回歸模型。</p>
<pre class="r"><code>d &lt;- read.table(&quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&quot;, sep = &quot;,&quot;, header = T)
head(d)</code></pre>
<pre><code>##   PersonID A Score  M  Y
## 1        1 0    69 43 38
## 2        2 1   145 56 40
## 3        3 0   125 32 24
## 4        4 1    86 45 33
## 5        5 1   158 33 23
## 6        6 0   133 61 60</code></pre>
<p>其中，</p>
<ul>
<li><code>PersonID</code>: 是學生的編號；</li>
<li><code>A</code>, <code>Score</code>: 用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 <code>A</code>，和表示對學習本身是否喜歡的評分 (滿分200)；</li>
<li><code>M</code>: 過去三個月內，該名學生一共需要上課的總課時數；</li>
<li><code>Y</code>: 過去三個月內，該名學生實際上出勤的課時數。</li>
</ul>
<p>這一次我們希望通過分析泊松回歸來回答「<code>A</code> 和 <code>Score</code> 對總課時數 <code>M</code> 具體有多大的影響？」這個問題。<a href="https://wangcc.me/post/logistic-rstan/">之前擬合貝葉斯邏輯回歸模型</a>時，使用的結果變量是 <code>Y</code>，也就是實際出勤課時數。但是本小節我們用 <code>M</code> 作爲結果變量。因爲總課時數是學生自己選課時的結果，也就是說學生本身的態度（是否喜歡打工，是否熱愛學習），可能本身左右了他/她到底會選多少課。背景知識假設是：喜歡多去打工的學生，選課可能態度消極，總課時數從開始可能就選的少。那麼像總選課時數這樣的非負（計數型）離散變量作爲結果變量的時候，<strong>泊松回歸模型是我們的第一選擇。</strong></p>
<div id="想象模型機制" class="section level2">
<h2>想象模型機制</h2>
<p>如果使用<a href="https://wangcc.me/post/rstan-wonderful-r3/">上上節介紹的多重線性回歸模型</a>，那麼模型的預測變量的分佈便可能取到負數，這樣就不符合實際情況下“總選課時數”是非負（計數型）離散變量這一事實。這就需要把預測變量 <code>A</code> 和 <code>Score</code> 相加的線性模型 <span class="math inline">\((b_1 + b_2A + b_3Score)\)</span>，通過數學轉換限制在非負數範圍。假設平均總課時數是 <span class="math inline">\(\lambda\)</span>，我們認爲它服從均值是 <span class="math inline">\(\lambda\)</span> 的泊松分佈。關於泊松分佈的詳細知識，期望值和方差的推導可以參考<a href="https://wangcc.me/LSHTMlearningnote/poisson.html">學習筆記</a>。另外，非貝葉斯版本的一般性傳統泊松回歸模型可以參照學習筆記的<a href="https://wangcc.me/LSHTMlearningnote/poisson-regression.html">廣義線性回歸的泊松回歸模型章節</a>。</p>
<p>對泊松回歸模型略有瞭解的話應該很自然地想到，把結果變量限制在非負數範圍的標準鏈接方程是 <span class="math inline">\(\log(\lambda)\)</span>，或者在 Stan 模型中，我們更自然地把線性模型部分寫在指數模型中: <span class="math inline">\(\exp(b_1 + b_2A + b_3Score)\)</span>。</p>
</div>
<div id="寫下數學模型表達式" class="section level2">
<h2>寫下數學模型表達式</h2>
<p><span class="math display">\[
\begin{aligned}
\lambda[n] &amp; = \exp(b_1 + b_2A[n] + b_3Score[n]) &amp; n = 1, \dots, N \\
M[n]       &amp; \sim \text{Poisson}(\lambda[n])     &amp; n = 1, \dots, N
\end{aligned}
\]</span></p>
<p>其中，</p>
<ul>
<li><span class="math inline">\(N\)</span>，是該數據中學生的人數；</li>
<li><span class="math inline">\(n\)</span>，是每名學生的標籤/編號（下標）；</li>
<li><span class="math inline">\(b_1, b_2, b_3\)</span> 是我們感興趣的參數。</li>
</ul>
</div>
<div id="把數學模型翻譯成-stan-模型代碼" class="section level2">
<h2>把數學模型翻譯成 Stan 模型代碼</h2>
<pre><code>data {
  int N; 
  int&lt;lower=0, upper=1&gt; A[N]; 
  real&lt;lower=0, upper=1&gt; Score[N]; 
  int&lt;lower=0&gt; M[N];
}

parameters {
  real b[3]; 
}

transformed parameters {
  real lambda[N];
  for (n in 1:N) {
    lambda[n] = exp(b[1] + b[2]*A[n] + b[3]*Score[n]);
  }
}

model {
  for (n in 1:N) {
    M[n] ~ poisson(lambda[n]); 
  }
}

generated quantities {
  int m_pred[N]; 
  for (n in 1:N) {
    m_pred[n] = poisson_rng(M[n], q[n]);
  }
}</code></pre>
<p>值得一提的是，在 Stan 中，提供了 <code>poisson_log(x)</code> 分佈函數，其實它等價於使用 <code>poisson(exp(x))</code>。除了更加接近我們熟悉的泊松回歸模型的數學表達式，避免了 <code>exp</code> 指數運算，計算結果穩定。於是我們還可以把上面的模型修改成：</p>
<pre><code>data {
  int N; 
  int&lt;lower=0, upper=1&gt; A[N]; 
  real&lt;lower=0, upper=1&gt; Score[N]; 
  int&lt;lower=0&gt; M[N];
}

parameters {
  real b[3]; 
}

transformed parameters {
  real lambda[N];
  for (n in 1:N) {
    lambda[n] = b[1] + b[2]*A[n] + b[3]*Score[n]；
  }
}

model {
  for (n in 1:N) {
    M[n] ~ poisson_log(lambda[n]); 
  }
}

generated quantities {
  int m_pred[N]; 
  for (n in 1:N) {
    m_pred[n] = poisson_log_rng(M[n], q[n]);
  }
}</code></pre>
<p>運行它的代碼如下：</p>
<pre class="r"><code>library(rstan)</code></pre>
<pre><code>## Loading required package: StanHeaders</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## rstan (Version 2.19.2, GitRev: 2e1f913d3ca3)</code></pre>
<pre><code>## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)</code></pre>
<pre class="r"><code>data &lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M)
# fit &lt;- stan(file=&#39;model/model5-6.stan&#39;, data=data, seed=1234)
fit &lt;- stan(file=&#39;stanfiles/model5-6b.stan&#39;, data=data, seed=1234, pars = c(&quot;b&quot;, &quot;lambda&quot;))</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;model5-6b&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.3e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.101417 seconds (Warm-up)
## Chain 1:                0.11273 seconds (Sampling)
## Chain 1:                0.214147 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;model5-6b&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 9e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.103552 seconds (Warm-up)
## Chain 2:                0.113478 seconds (Sampling)
## Chain 2:                0.21703 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;model5-6b&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 6e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.106149 seconds (Warm-up)
## Chain 3:                0.103742 seconds (Sampling)
## Chain 3:                0.209891 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;model5-6b&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.10037 seconds (Warm-up)
## Chain 4:                0.11621 seconds (Sampling)
## Chain 4:                0.21658 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>fit</code></pre>
<pre><code>## Inference for Stan model: model5-6b.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##               mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff
## b[1]          3.58    0.00 0.10    3.38    3.51    3.58    3.64    3.76  1187
## b[2]          0.26    0.00 0.04    0.18    0.24    0.26    0.29    0.35  1831
## b[3]          0.29    0.00 0.15    0.01    0.19    0.29    0.39    0.58  1227
## lambda[1]     3.68    0.00 0.05    3.58    3.65    3.68    3.71    3.77  1276
## lambda[2]     4.05    0.00 0.03    3.98    4.03    4.05    4.07    4.12  2189
## lambda[3]     3.76    0.00 0.03    3.70    3.74    3.76    3.78    3.81  2467
## lambda[4]     3.97    0.00 0.04    3.88    3.94    3.97    3.99    4.05  1735
## lambda[5]     4.07    0.00 0.04    3.99    4.04    4.07    4.10    4.15  1793
## lambda[6]     3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.83  2509
## lambda[7]     3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1991
## lambda[8]     4.05    0.00 0.03    3.98    4.03    4.06    4.08    4.12  2102
## lambda[9]     3.79    0.00 0.03    3.72    3.77    3.79    3.81    3.85  2282
## lambda[10]    3.79    0.00 0.03    3.72    3.77    3.79    3.81    3.85  2305
## lambda[11]    4.05    0.00 0.03    3.98    4.02    4.05    4.07    4.11  2347
## lambda[12]    3.78    0.00 0.03    3.72    3.76    3.78    3.80    3.83  2465
## lambda[13]    4.01    0.00 0.03    3.95    3.99    4.01    4.03    4.07  2499
## lambda[14]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1991
## lambda[15]    3.73    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1910
## lambda[16]    3.98    0.00 0.04    3.91    3.96    3.98    4.01    4.05  2088
## lambda[17]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.80  2122
## lambda[18]    3.70    0.00 0.04    3.61    3.67    3.70    3.72    3.78  1360
## lambda[19]    3.85    0.00 0.05    3.74    3.81    3.85    3.88    3.95  1653
## lambda[20]    4.07    0.00 0.04    3.99    4.04    4.07    4.09    4.14  1835
## lambda[21]    3.97    0.00 0.04    3.88    3.94    3.97    3.99    4.05  1735
## lambda[22]    4.00    0.00 0.03    3.93    3.98    4.00    4.02    4.06  2298
## lambda[23]    3.99    0.00 0.03    3.93    3.97    3.99    4.02    4.06  2252
## lambda[24]    4.05    0.00 0.03    3.98    4.03    4.05    4.07    4.11  2283
## lambda[25]    4.01    0.00 0.03    3.95    3.99    4.01    4.03    4.07  2483
## lambda[26]    4.03    0.00 0.03    3.97    4.01    4.03    4.05    4.09  2548
## lambda[27]    3.75    0.00 0.03    3.69    3.73    3.75    3.77    3.80  2321
## lambda[28]    3.75    0.00 0.03    3.69    3.73    3.75    3.77    3.80  2321
## lambda[29]    3.81    0.00 0.04    3.73    3.78    3.81    3.84    3.89  1971
## lambda[30]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.80  2077
## lambda[31]    4.08    0.00 0.04    4.00    4.05    4.08    4.11    4.17  1672
## lambda[32]    3.95    0.00 0.05    3.85    3.92    3.95    3.98    4.04  1571
## lambda[33]    4.04    0.00 0.03    3.98    4.02    4.04    4.06    4.10  2409
## lambda[34]    4.02    0.00 0.03    3.96    4.00    4.02    4.04    4.08  2554
## lambda[35]    3.76    0.00 0.03    3.70    3.74    3.76    3.78    3.81  2482
## lambda[36]    3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.82  2516
## lambda[37]    3.99    0.00 0.03    3.93    3.97    3.99    4.02    4.06  2229
## lambda[38]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1950
## lambda[39]    3.71    0.00 0.04    3.63    3.68    3.71    3.73    3.78  1437
## lambda[40]    3.70    0.00 0.04    3.63    3.68    3.70    3.73    3.78  1424
## lambda[41]    3.76    0.00 0.03    3.71    3.75    3.76    3.78    3.82  2512
## lambda[42]    3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.83  2509
## lambda[43]    3.75    0.00 0.03    3.70    3.74    3.75    3.77    3.81  2404
## lambda[44]    3.79    0.00 0.03    3.72    3.77    3.79    3.82    3.86  2236
## lambda[45]    3.84    0.00 0.05    3.74    3.81    3.84    3.88    3.95  1683
## lambda[46]    3.73    0.00 0.03    3.67    3.71    3.73    3.75    3.79  1772
## lambda[47]    3.65    0.00 0.06    3.53    3.61    3.65    3.69    3.77  1222
## lambda[48]    3.79    0.00 0.04    3.73    3.77    3.79    3.82    3.86  2191
## lambda[49]    3.72    0.00 0.03    3.65    3.70    3.72    3.74    3.79  1613
## lambda[50]    3.98    0.00 0.04    3.91    3.96    3.98    4.01    4.05  2088
## lp__       6896.53    0.03 1.28 6893.17 6895.97 6896.87 6897.45 6897.95  1452
##            Rhat
## b[1]          1
## b[2]          1
## b[3]          1
## lambda[1]     1
## lambda[2]     1
## lambda[3]     1
## lambda[4]     1
## lambda[5]     1
## lambda[6]     1
## lambda[7]     1
## lambda[8]     1
## lambda[9]     1
## lambda[10]    1
## lambda[11]    1
## lambda[12]    1
## lambda[13]    1
## lambda[14]    1
## lambda[15]    1
## lambda[16]    1
## lambda[17]    1
## lambda[18]    1
## lambda[19]    1
## lambda[20]    1
## lambda[21]    1
## lambda[22]    1
## lambda[23]    1
## lambda[24]    1
## lambda[25]    1
## lambda[26]    1
## lambda[27]    1
## lambda[28]    1
## lambda[29]    1
## lambda[30]    1
## lambda[31]    1
## lambda[32]    1
## lambda[33]    1
## lambda[34]    1
## lambda[35]    1
## lambda[36]    1
## lambda[37]    1
## lambda[38]    1
## lambda[39]    1
## lambda[40]    1
## lambda[41]    1
## lambda[42]    1
## lambda[43]    1
## lambda[44]    1
## lambda[45]    1
## lambda[46]    1
## lambda[47]    1
## lambda[48]    1
## lambda[49]    1
## lambda[50]    1
## lp__          1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jan  7 14:46:49 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
<div id="運行結果的解釋" class="section level2">
<h2>運行結果的解釋</h2>
<pre><code>...{省略}...
              mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
b[1]          3.58    0.00 0.09    3.38    3.51    3.58    3.64    3.76  1373    1
b[2]          0.26    0.00 0.04    0.18    0.24    0.26    0.29    0.35  1797    1
b[3]          0.29    0.00 0.15    0.00    0.20    0.29    0.39    0.59  1422    1
lambda[1]     3.68    0.00 0.05    3.58    3.65    3.68    3.71    3.77  1510    1
...{省略}...</code></pre>
<p>我們把計算獲得的事後概率分佈均值放入前面寫下的數學表達式:</p>
<p><span class="math display">\[
\begin{aligned}
\lambda[n] &amp; = \exp(3.58 + 0.26A[n] + 0.29Score[n]/200) &amp; n = 1, \dots, N \\
M[n]       &amp; \sim \text{Poisson}(\lambda[n])     &amp; n = 1, \dots, N
\end{aligned}
\]</span></p>
<p>例如說，<code>Score = 150</code> 和 <code>Score = 50</code> 的兩名學生，如果對打工喜好態度相同的話，他們之間選課的總課時數之比爲：</p>
<p><span class="math display">\[
\begin{aligned}
\frac{M_\text{Score = 150}}{M_\text{Score = 50}} &amp; = \frac{\exp(3.58 + 0.26A + 0.29\times\frac{150}{200})}{\exp(3.58 + 0.26A + 0.29\times\frac{50}{200})} \\ 
&amp; = \exp(0.29\times\frac{150-50}{200}) \approx 1.16
\end{aligned}
\]</span></p>
<p>也就是熱愛學習分數 <code>Score</code> 達到150的人和只有50的人相比，選課總課時數平均多 16%。相似地，喜歡打工 <code>A = 1</code> 的學生和不喜歡打工 <code>A = 0</code> 的學生選課總課時數之比爲 <span class="math inline">\(\exp(0.26)\approx1.30\)</span>。</p>
</div>
</div>
