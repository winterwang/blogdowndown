---
title: Rstan Wonderful R-(4)
author: ''
date: '2019-01-27'
slug: logistic-rstan
# slug: rstan-wonderful-r(3)
categories:
  - Bayesian
  - R techniques
  - statistics
tags:
  - Bayesian
  - Medical Statistics
header:
  caption: ''
  image: '052816_bayesian-opener_free.jpg'
output:
  blogdown::html_page:
    toc: true
summary: "Rstan 學習筆記 Chapter 5.2"
---


<div id="TOC">
<ul>
<li><a href="#邏輯回歸模型的-rstan-貝葉斯實現">邏輯回歸模型的 Rstan 貝葉斯實現</a></li>
<li><a href="#確定分析目的">確定分析目的</a></li>
<li><a href="#確認數據分佈">確認數據分佈</a></li>
<li><a href="#寫下數學模型表達式">寫下數學模型表達式</a></li>
<li><a href="#確認收斂效果">確認收斂效果</a></li>
</ul>
</div>

<div id="邏輯回歸模型的-rstan-貝葉斯實現" class="section level1">
<h1>邏輯回歸模型的 Rstan 貝葉斯實現</h1>
<p>本小節使用的<a href="https://raw.githubusercontent.com/MatsuuraKentaro/RStanBook/master/chap05/input/data-attendance-2.txt">數據</a>，和前一節的出勤率數據很類似:</p>
<pre class="r"><code>d &lt;- read.table(&quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&quot;, 
                     sep = &quot;,&quot;, header = T)
head(d)</code></pre>
<pre><code>##   PersonID A Score  M  Y
## 1        1 0    69 43 38
## 2        2 1   145 56 40
## 3        3 0   125 32 24
## 4        4 1    86 45 33
## 5        5 1   158 33 23
## 6        6 0   133 61 60</code></pre>
<p>其中，</p>
<ul>
<li><code>PersonID</code>: 是學生的編號；</li>
<li><code>A</code>, <code>Score</code>: 和之前一樣用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 <code>A</code>，和表示對學習本身是否喜歡的評分 (滿分200)；</li>
<li><code>M</code>: 過去三個月內，該名學生一共需要上課的總課時數；</li>
<li><code>Y</code>: 過去三個月內，該名學生實際上出勤的課時數。</li>
</ul>
</div>
<div id="確定分析目的" class="section level1">
<h1>確定分析目的</h1>
<p>需要回答的問題依然是，<span class="math inline">\(A, Score\)</span> 分別在多大程度上預測學生的出勤率？另外，我們希望知道的是，當需要修的課時數固定的事後，這兩個預測變量能準確提供 <span class="math inline">\(Y\)</span> 的多少信息？</p>
</div>
<div id="確認數據分佈" class="section level1">
<h1>確認數據分佈</h1>
<pre class="r"><code>library(ggplot2)
library(GGally)

set.seed(1)
d &lt;- d[, -1]
# d &lt;- read.csv(file=&#39;input/data-attendance-2.txt&#39;)[,-1]
d$A &lt;- as.factor(d$A)
d &lt;- transform(d, ratio=Y/M)
N_col &lt;- ncol(d)
ggp &lt;- ggpairs(d, upper=&#39;blank&#39;, diag=&#39;blank&#39;, lower=&#39;blank&#39;)

for(i in 1:N_col) {
  x &lt;- d[,i]
  p &lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &lt;- p + theme_bw(base_size=14)
  p &lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &#39;factor&#39;) {
    p &lt;- p + geom_bar(aes(fill=A), color=&#39;grey20&#39;)
  } else {
    bw &lt;- (max(x)-min(x))/10
    p &lt;- p + geom_histogram(aes(fill=A), color=&#39;grey20&#39;, binwidth=bw)
    p &lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&#39;density&#39;)
  }
  p &lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &lt;- p + scale_fill_manual(values=alpha(c(&#39;white&#39;, &#39;grey40&#39;), 0.5))
  ggp &lt;- putPlot(ggp, p, i, i)
}

zcolat &lt;- seq(-1, 1, length=81)
zcolre &lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &lt;- as.numeric(d[,i])
    y &lt;- as.numeric(d[,j])
    r &lt;- cor(x, y, method=&#39;spearman&#39;, use=&#39;pairwise.complete.obs&#39;)
    zcol &lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &lt;- ifelse(abs(r) &lt; 0.4, &#39;grey20&#39;, &#39;white&#39;)
    ell &lt;- ellipse::ellipse(r, level=0.95, type=&#39;l&#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &lt;- d[,j]
    y &lt;- d[,i]
    p &lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &lt;- p + theme_bw(base_size=14)
    p &lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &#39;factor&#39;) {
      p &lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&#39;white&#39;)
      p &lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &lt;- p + geom_point(size=2)
    }
    p &lt;- p + scale_shape_manual(values=c(21, 24))
    p &lt;- p + scale_fill_manual(values=alpha(c(&#39;white&#39;, &#39;grey40&#39;), 0.5))
    ggp &lt;- putPlot(ggp, p, i, j)
  }
}

ggp</code></pre>
<div class="figure" style="text-align: center"><span id="fig:step1"></span>
<img src="/post/2019-01-27-logistic-rstan_files/figure-html/step1-1.png" alt="三個變量的分佈觀察圖，相比之前增加了 $ratio = Y/M$ 列。" width="80%" />
<p class="caption">
Figure 1: 三個變量的分佈觀察圖，相比之前增加了 <span class="math inline">\(ratio = Y/M\)</span> 列。
</p>
</div>
<p>從圖 <a href="#fig:step1">1</a> 還可以看出，由於總課時數越多，學生實際出勤的課時數也會越多所以 <span class="math inline">\(M, Y\)</span> 兩者之間理應有很強的正相關。另外可能可以推測的是 <span class="math inline">\(Ratio\)</span> 和是否愛學習的分數之間大概有可能有正相關，和是否喜歡打工之間大概可能有負相關。</p>
</div>
<div id="寫下數學模型表達式" class="section level1">
<h1>寫下數學模型表達式</h1>
<p>在 Stan 的語法中，使用的是反邏輯函數 (inverse logit): <code>inv_logit</code> 來描述下面的邏輯回歸模型 5-4。</p>
<p><span class="math display">\[
\begin{array}{l}
q[n] = \text{inv_logit}(b_1 + b_2 A[n] + b_3Score[n]) &amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp; n = 1, 2, \dots, N \\
\end{array}
\]</span></p>
<p>上面的數學模型，可以被翻譯成下面的 Stan 語言:</p>
<pre><code>data {
  int N; 
  int&lt;lower=0, upper=1&gt; A[N]; 
  real&lt;lower=0, upper=1&gt; Score[N]; 
  int&lt;lower=0&gt; M[N];
  int&lt;lower=0&gt; Y[N];
}

parameters {
  real b1; 
  real b2; 
  real b3;
}

transformed parameters {
  real q[N];
  for (n in 1:N) {
    q[n] = inv_logit(b1 + b2*A[n] + b3*Score[n]);
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ binomial(M[n], q[n]); 
  }
}

generated quantities {
  real y_pred[N]; 
  for (n in 1:N) {
    y_pred[n] = binomial_rng(M[n], q[n]);
  }
}
</code></pre>
<p>下面的 R 代碼用來實現對上面 Stan 模型的擬合:</p>
<pre class="r"><code>library(rstan)
d &lt;- read.csv(file=&#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&#39;, header = T)
data &lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M, Y=d$Y)
fit &lt;- stan(file=&#39;stanfiles/model5-4.stan&#39;, data=data, seed=1234)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;model5-4&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.5e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.129287 seconds (Warm-up)
## Chain 1:                0.13611 seconds (Sampling)
## Chain 1:                0.265397 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;model5-4&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 8e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.131371 seconds (Warm-up)
## Chain 2:                0.133643 seconds (Sampling)
## Chain 2:                0.265014 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;model5-4&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 8e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.128284 seconds (Warm-up)
## Chain 3:                0.133711 seconds (Sampling)
## Chain 3:                0.261995 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;model5-4&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 8e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.128226 seconds (Warm-up)
## Chain 4:                0.140913 seconds (Sampling)
## Chain 4:                0.269139 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>fit</code></pre>
<pre><code>## Inference for Stan model: model5-4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## b1             0.09    0.01 0.22    -0.37    -0.05     0.09     0.24     0.53
## b2            -0.62    0.00 0.09    -0.80    -0.68    -0.62    -0.56    -0.44
## b3             1.90    0.01 0.36     1.19     1.67     1.90     2.13     2.64
## q[1]           0.68    0.00 0.02     0.63     0.66     0.68     0.69     0.72
## q[2]           0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[3]           0.78    0.00 0.01     0.76     0.77     0.78     0.79     0.80
## q[4]           0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[5]           0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[6]           0.79    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[7]           0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[8]           0.70    0.00 0.02     0.67     0.69     0.70     0.72     0.74
## q[9]           0.81    0.00 0.01     0.79     0.81     0.81     0.82     0.84
## q[10]          0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[11]          0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[12]          0.80    0.00 0.01     0.78     0.79     0.80     0.81     0.82
## q[13]          0.64    0.00 0.01     0.62     0.63     0.64     0.65     0.67
## q[14]          0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[15]          0.76    0.00 0.01     0.73     0.75     0.76     0.76     0.78
## q[16]          0.60    0.00 0.02     0.57     0.59     0.60     0.61     0.64
## q[17]          0.76    0.00 0.01     0.74     0.76     0.76     0.77     0.79
## q[18]          0.70    0.00 0.02     0.66     0.69     0.71     0.72     0.74
## q[19]          0.86    0.00 0.02     0.83     0.85     0.86     0.87     0.89
## q[20]          0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[21]          0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[22]          0.62    0.00 0.02     0.59     0.61     0.62     0.63     0.65
## q[23]          0.62    0.00 0.02     0.59     0.61     0.62     0.63     0.65
## q[24]          0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[25]          0.64    0.00 0.01     0.61     0.63     0.64     0.65     0.67
## q[26]          0.67    0.00 0.01     0.64     0.66     0.67     0.68     0.69
## q[27]          0.77    0.00 0.01     0.75     0.76     0.77     0.78     0.79
## q[28]          0.77    0.00 0.01     0.75     0.76     0.77     0.78     0.79
## q[29]          0.83    0.00 0.01     0.81     0.83     0.84     0.84     0.86
## q[30]          0.76    0.00 0.01     0.74     0.75     0.76     0.77     0.79
## q[31]          0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[32]          0.54    0.00 0.03     0.49     0.53     0.54     0.56     0.60
## q[33]          0.69    0.00 0.01     0.66     0.68     0.69     0.70     0.72
## q[34]          0.66    0.00 0.01     0.63     0.65     0.66     0.67     0.69
## q[35]          0.78    0.00 0.01     0.76     0.78     0.78     0.79     0.81
## q[36]          0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.81
## q[37]          0.62    0.00 0.02     0.58     0.60     0.62     0.63     0.65
## q[38]          0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[39]          0.72    0.00 0.02     0.68     0.71     0.72     0.73     0.75
## q[40]          0.72    0.00 0.02     0.68     0.70     0.72     0.73     0.75
## q[41]          0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[42]          0.79    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[43]          0.78    0.00 0.01     0.75     0.77     0.78     0.79     0.80
## q[44]          0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[45]          0.86    0.00 0.02     0.83     0.85     0.86     0.87     0.89
## q[46]          0.75    0.00 0.01     0.72     0.74     0.75     0.76     0.77
## q[47]          0.64    0.00 0.03     0.57     0.62     0.64     0.66     0.70
## q[48]          0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[49]          0.74    0.00 0.01     0.71     0.73     0.74     0.75     0.76
## q[50]          0.60    0.00 0.02     0.57     0.59     0.60     0.61     0.64
## y_pred[1]     29.19    0.05 3.21    23.00    27.00    29.00    31.00    35.00
## y_pred[2]     39.25    0.06 3.56    32.00    37.00    39.00    42.00    46.00
## y_pred[3]     25.07    0.04 2.38    20.00    24.00    25.00    27.00    29.00
## y_pred[4]     25.76    0.06 3.43    19.00    23.00    26.00    28.00    32.00
## y_pred[5]     23.89    0.04 2.62    19.00    22.00    24.00    26.00    29.00
## y_pred[6]     48.46    0.05 3.24    42.00    46.00    49.00    51.00    54.00
## y_pred[7]     37.21    0.05 3.10    31.00    35.00    37.00    39.00    43.00
## y_pred[8]     53.44    0.07 4.12    45.00    51.00    54.00    56.00    61.00
## y_pred[9]     63.52    0.06 3.54    56.00    61.00    64.00    66.00    70.00
## y_pred[10]    51.98    0.05 3.21    45.00    50.00    52.00    54.00    58.00
## y_pred[11]    23.50    0.05 2.73    18.00    22.00    24.00    25.00    29.00
## y_pred[12]    35.26    0.04 2.68    30.00    33.00    35.00    37.00    40.00
## y_pred[13]    34.17    0.06 3.53    27.00    32.00    34.00    37.00    41.00
## y_pred[14]    30.39    0.04 2.73    25.00    29.00    30.00    32.00    35.00
## y_pred[15]    42.23    0.05 3.23    36.00    40.00    42.00    44.00    48.00
## y_pred[16]    35.48    0.06 3.88    28.00    33.00    36.00    38.00    43.00
## y_pred[17]    28.97    0.04 2.69    23.00    27.00    29.00    31.00    34.00
## y_pred[18]    31.67    0.05 3.12    25.00    30.00    32.00    34.00    38.00
## y_pred[19]    38.81    0.04 2.39    34.00    37.00    39.00    40.00    43.00
## y_pred[20]    55.48    0.07 4.26    47.00    53.00    56.00    58.00    63.00
## y_pred[21]    40.16    0.07 4.39    32.00    37.00    40.00    43.00    49.00
## y_pred[22]    47.97    0.08 4.48    39.00    45.00    48.00    51.00    56.03
## y_pred[23]    38.94    0.06 3.89    32.00    36.00    39.00    42.00    46.00
## y_pred[24]    47.35    0.06 3.87    40.00    45.00    47.00    50.00    55.00
## y_pred[25]    32.10    0.06 3.42    25.00    30.00    32.00    34.00    39.00
## y_pred[26]    34.02    0.05 3.38    27.00    32.00    34.00    36.00    40.00
## y_pred[27]    22.42    0.04 2.29    18.00    21.00    23.00    24.00    27.00
## y_pred[28]    28.59    0.04 2.57    23.00    27.00    29.00    30.00    33.00
## y_pred[29]    15.08    0.02 1.59    12.00    14.00    15.00    16.00    18.00
## y_pred[30]    37.37    0.05 3.02    31.00    35.00    37.00    39.00    43.00
## y_pred[31]    55.42    0.07 4.05    47.00    53.00    56.00    58.00    63.00
## y_pred[32]     6.50    0.03 1.75     3.00     5.00     7.00     8.00    10.00
## y_pred[33]    15.82    0.04 2.24    11.00    14.00    16.00    17.00    20.00
## y_pred[34]    24.33    0.05 2.88    19.00    22.00    24.00    26.00    30.00
## y_pred[35]    46.26    0.05 3.27    39.00    44.00    46.00    49.00    52.00
## y_pred[36]    43.51    0.05 3.06    37.00    41.75    44.00    46.00    49.00
## y_pred[37]    54.28    0.08 4.82    45.00    51.00    54.00    58.00    63.00
## y_pred[38]    35.60    0.05 3.04    29.00    34.00    36.00    38.00    41.00
## y_pred[39]    15.84    0.04 2.19    11.00    14.00    16.00    17.00    20.00
## y_pred[40]    29.39    0.05 2.98    23.00    27.00    29.00    31.00    35.00
## y_pred[41]    45.05    0.05 3.14    39.00    43.00    45.00    47.00    51.00
## y_pred[42]    25.43    0.04 2.34    21.00    24.00    26.00    27.00    30.00
## y_pred[43]    41.18    0.05 3.07    35.00    39.00    41.00    43.00    47.00
## y_pred[44]    25.37    0.03 2.16    21.00    24.00    25.00    27.00    29.00
## y_pred[45]    19.76    0.03 1.74    16.00    19.00    20.00    21.00    23.00
## y_pred[46]    38.21    0.05 3.19    32.00    36.00    38.00    40.00    44.00
## y_pred[47]    14.09    0.04 2.37     9.00    12.00    14.00    16.00    18.00
## y_pred[48]    31.20    0.04 2.41    26.00    30.00    31.00    33.00    36.00
## y_pred[49]    16.90    0.03 2.12    12.00    16.00    17.00    18.00    21.00
## y_pred[50]    40.34    0.07 4.25    32.00    37.00    40.00    43.00    48.00
## lp__       -1389.32    0.03 1.20 -1392.42 -1389.91 -1389.01 -1388.42 -1387.95
##            n_eff Rhat
## b1          1443    1
## b2          2052    1
## b3          1519    1
## q[1]        1523    1
## q[2]        2412    1
## q[3]        2751    1
## q[4]        2023    1
## q[5]        2065    1
## q[6]        2718    1
## q[7]        2352    1
## q[8]        2345    1
## q[9]        2443    1
## q[10]       2467    1
## q[11]       2561    1
## q[12]       2648    1
## q[13]       3089    1
## q[14]       2352    1
## q[15]       2267    1
## q[16]       2372    1
## q[17]       2483    1
## q[18]       1617    1
## q[19]       1891    1
## q[20]       2106    1
## q[21]       2023    1
## q[22]       2727    1
## q[23]       2642    1
## q[24]       2484    1
## q[25]       3065    1
## q[26]       3059    1
## q[27]       2648    1
## q[28]       2648    1
## q[29]       2149    1
## q[30]       2439    1
## q[31]       1948    1
## q[32]       1839    1
## q[33]       2682    1
## q[34]       3131    1
## q[35]       2757    1
## q[36]       2743    1
## q[37]       2600    1
## q[38]       2309    1
## q[39]       1707    1
## q[40]       1691    1
## q[41]       2757    1
## q[42]       2718    1
## q[43]       2713    1
## q[44]       2398    1
## q[45]       1913    1
## q[46]       2111    1
## q[47]       1470    1
## q[48]       2354    1
## q[49]       1919    1
## q[50]       2372    1
## y_pred[1]   3738    1
## y_pred[2]   3669    1
## y_pred[3]   3582    1
## y_pred[4]   3362    1
## y_pred[5]   3706    1
## y_pred[6]   3914    1
## y_pred[7]   3706    1
## y_pred[8]   3447    1
## y_pred[9]   3934    1
## y_pred[10]  3907    1
## y_pred[11]  3488    1
## y_pred[12]  4354    1
## y_pred[13]  3685    1
## y_pred[14]  3854    1
## y_pred[15]  3715    1
## y_pred[16]  3841    1
## y_pred[17]  3972    1
## y_pred[18]  3780    1
## y_pred[19]  3683    1
## y_pred[20]  3505    1
## y_pred[21]  3841    1
## y_pred[22]  3432    1
## y_pred[23]  3919    1
## y_pred[24]  3814    1
## y_pred[25]  3828    1
## y_pred[26]  3793    1
## y_pred[27]  3778    1
## y_pred[28]  4052    1
## y_pred[29]  4096    1
## y_pred[30]  3820    1
## y_pred[31]  3352    1
## y_pred[32]  3583    1
## y_pred[33]  3970    1
## y_pred[34]  3628    1
## y_pred[35]  3789    1
## y_pred[36]  3968    1
## y_pred[37]  3355    1
## y_pred[38]  3092    1
## y_pred[39]  3815    1
## y_pred[40]  3738    1
## y_pred[41]  4172    1
## y_pred[42]  4290    1
## y_pred[43]  3624    1
## y_pred[44]  4023    1
## y_pred[45]  3531    1
## y_pred[46]  3976    1
## y_pred[47]  3218    1
## y_pred[48]  3811    1
## y_pred[49]  3951    1
## y_pred[50]  3826    1
## lp__        1381    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jan  7 14:48:25 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>把獲得的參數事後樣本的均值代入上面的數學模型中可得:</p>
<p><span class="math display">\[
\begin{array}{l}
q[n] = \text{inv_logit}(0.09 - 0.62 A[n] + 1.90Score[n]) &amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp; n = 1, 2, \dots, N \\
\end{array}
\]</span></p>
</div>
<div id="確認收斂效果" class="section level1">
<h1>確認收斂效果</h1>
<pre class="r"><code>library(bayesplot)

color_scheme_set(&quot;mix-brightblue-gray&quot;)

posterior2 &lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;lp__&quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p</code></pre>
<div class="figure" style="text-align: center"><span id="fig:step53"></span>
<img src="/post/2019-01-27-logistic-rstan_files/figure-html/step53-1.png" alt="用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。" width="80%" />
<p class="caption">
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
</p>
</div>
<pre class="r"><code>ms &lt;- rstan::extract(fit)

d_qua &lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &lt;- c(&#39;p10&#39;, &#39;p50&#39;, &#39;p90&#39;)
d_qua &lt;- data.frame(d, d_qua)
d_qua$A &lt;- as.factor(d_qua$A)

p &lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&#39;line&#39;))
p &lt;- p + coord_fixed(ratio=1, xlim=c(5, 70), ylim=c(5, 70))
p &lt;- p + geom_pointrange(size=0.8, color=&#39;grey5&#39;)
p &lt;- p + geom_abline(aes(slope=1, intercept=0), color=&#39;black&#39;, alpha=3/5, linetype=&#39;31&#39;)
p &lt;- p + scale_shape_manual(values=c(21, 24))
p &lt;- p + scale_fill_manual(values=c(&#39;white&#39;, &#39;grey70&#39;))
p &lt;- p + labs(x=&#39;Observed&#39;, y=&#39;Predicted&#39;)
p &lt;- p + scale_x_continuous(breaks=seq(from=0, to=70, by=20))
p &lt;- p + scale_y_continuous(breaks=seq(from=0, to=70, by=20))
p</code></pre>
<div class="figure" style="text-align: center"><span id="fig:fig58"></span>
<img src="/post/2019-01-27-logistic-rstan_files/figure-html/fig58-1.png" alt="觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。" width="80%" />
<p class="caption">
Figure 3: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
</p>
</div>
</div>
